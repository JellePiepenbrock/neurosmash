{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE",
      "provenance": [],
      "collapsed_sections": [
        "mBu3MfoI3uqu",
        "Mz63VjN8I1Wa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JellePiepenbrock/neurosmash/blob/master/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_91uizYrFu6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import os, sys\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "sys.path.append('/content/gdrive/My Drive/projects/entity_tagging/')\n",
        "\n",
        "base_url = '/content//gdrive/My Drive/projects/NIPS/neurosmash'\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk6A_Pnt36c9",
        "colab_type": "text"
      },
      "source": [
        "## Beta VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1bTDW4g35_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"model.py\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def reparametrize(mu, logvar):\n",
        "    std = logvar.div(2).exp()\n",
        "    eps = Variable(std.data.new(std.size()).normal_())\n",
        "    return mu + std*eps\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super(View, self).__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        return tensor.view(self.size)\n",
        "\n",
        "\n",
        "class BetaVAE_H(nn.Module):\n",
        "    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017).\"\"\"\n",
        "\n",
        "    def __init__(self, z_dim=10, nc=3):\n",
        "        super(BetaVAE_H, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.nc = nc\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1\n",
        "            nn.ReLU(True),\n",
        "            View((-1, 256*1*1)),                 # B, 256\n",
        "            nn.Linear(256, z_dim*2),             # B, z_dim*2\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),               # B, 256\n",
        "            View((-1, 256, 1, 1)),               # B, 256,  1,  1\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, nc, 4, 2, 1),  # B, nc, 64, 64\n",
        "        )\n",
        "\n",
        "        self.weight_init()\n",
        "\n",
        "    def weight_init(self):\n",
        "        for block in self._modules:\n",
        "            for m in self._modules[block]:\n",
        "                kaiming_init(m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        distributions = self._encode(x)\n",
        "        mu = distributions[:, :self.z_dim]\n",
        "        logvar = distributions[:, self.z_dim:]\n",
        "        z = reparametrize(mu, logvar)\n",
        "        x_recon = self._decode(z)\n",
        "\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "    def _encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def _decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "class BetaVAE_B(BetaVAE_H):\n",
        "    \"\"\"Model proposed in understanding beta-VAE paper(Burgess et al, arxiv:1804.03599, 2018).\"\"\"\n",
        "\n",
        "    def __init__(self, z_dim=10, nc=1):\n",
        "        super(BetaVAE_B, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32,  4,  4\n",
        "            nn.ReLU(True),\n",
        "            View((-1, 32*4*4)),                  # B, 512\n",
        "            nn.Linear(32*4*4, 256),              # B, 256\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 256),                 # B, 256\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, z_dim*2),             # B, z_dim*2\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),               # B, 256\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 256),                 # B, 256\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 32*4*4),              # B, 512\n",
        "            nn.ReLU(True),\n",
        "            View((-1, 32, 4, 4)),                # B,  32,  4,  4\n",
        "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, nc, 4, 2, 1), # B,  nc, 64, 64\n",
        "        )\n",
        "        self.weight_init()\n",
        "\n",
        "    def weight_init(self):\n",
        "        for block in self._modules:\n",
        "            for m in self._modules[block]:\n",
        "                normal_init(m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        distributions = self._encode(x)\n",
        "        mu = distributions[:, :self.z_dim]\n",
        "        logvar = distributions[:, self.z_dim:]\n",
        "        z = reparametrize(mu, logvar)\n",
        "        x_recon = self._decode(z).view(x.size())\n",
        "\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "    def _encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def _decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "def kaiming_init(m):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        init.kaiming_normal(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
        "        m.weight.data.fill_(1)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        if m.bias.data is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "        m.weight.data.fill_(1)\n",
        "        if m.bias.data is not None:\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MihaYYvp38pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhOb06em4TuH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "10bdd126-67d2-4b95-b354-8ec5dfde6562"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import Adam\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "from random import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from skimage import io, transform\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "device = torch.device('cuda')\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "objective = 'H'\n",
        "beta = 10\n",
        "lr =  1e-4\n",
        "z_dim =  32 \n",
        "max_iter =  1.5e6\n",
        "decoder_dist = 'gaussian'\n",
        "\n",
        "vae = BetaVAE_B(nc=3, z_dim=z_dim) if objective == 'B' else BetaVAE_H(nc=3)\n",
        "vae = vae.to(device)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "print('Loaded model.')\n",
        "\n",
        "# https://github.com/1Konny/Beta-VAE/blob/master/solver.py \n",
        "\n",
        "\n",
        "def reconstruction_loss(x, x_recon, distribution):\n",
        "    batch_size = x.size(0)\n",
        "    assert batch_size != 0\n",
        "\n",
        "    if distribution == 'bernoulli':\n",
        "        recon_loss = F.binary_cross_entropy_with_logits(x_recon, x, size_average=False).div(batch_size)\n",
        "    elif distribution == 'gaussian':\n",
        "        x_recon = F.sigmoid(x_recon)\n",
        "        recon_loss = F.mse_loss(x_recon, x, size_average=False).div(batch_size)\n",
        "    else:\n",
        "        recon_loss = None\n",
        "\n",
        "    return recon_loss\n",
        "\n",
        "\n",
        "def kl_divergence(mu, logvar):\n",
        "    batch_size = mu.size(0)\n",
        "    assert batch_size != 0\n",
        "    if mu.data.ndimension() == 4:\n",
        "        mu = mu.view(mu.size(0), mu.size(1))\n",
        "    if logvar.data.ndimension() == 4:\n",
        "        logvar = logvar.view(logvar.size(0), logvar.size(1))\n",
        "\n",
        "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    total_kld = klds.sum(1).mean(0, True)\n",
        "    dimension_wise_kld = klds.mean(0)\n",
        "    mean_kld = klds.mean(1).mean(0, True)\n",
        "\n",
        "    return total_kld, dimension_wise_kld, mean_kld\n",
        "\n",
        "\n",
        "def compare(x):\n",
        "    recon_x, _, _ = vae(x)\n",
        "    return torch.cat([x, recon_x])\n",
        "\n",
        "def main(episodes):\n",
        "    dataset = torch.load('{}/file.pt'.format(base_url)) / 255\n",
        "    # dataset = torch.from_numpy(transform.resize(dataset, (len(dataset), 32, 3, 32, 32)))\n",
        "    # print(dataset.shape)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset.reshape(32*len(dataset), 3, 64, 64), batch_size=batch_size, shuffle=True)\n",
        "    epochs = 500\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for x in dataloader:\n",
        "            x = x.to(device)\n",
        "            x_recon, mu, logvar = vae(x)\n",
        "            recon_loss = reconstruction_loss(x, x_recon, decoder_dist)\n",
        "            total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
        "\n",
        "            if objective == 'H':\n",
        "                beta_vae_loss = recon_loss + beta*total_kld\n",
        "            # elif objective == 'B':\n",
        "            #     C = torch.clamp(self.C_max/self.C_stop_iter*self.global_iter, 0, self.C_max.data[0])\n",
        "            #     beta_vae_loss = recon_loss + self.gamma*(total_kld-C).abs()\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            beta_vae_loss.backward()\n",
        "            optimizer.step()\n",
        "        if epoch == 100:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 1e-5\n",
        "        # Store one image per epoch to showcase development of VAE.\n",
        "        if (epoch % 10 == 0) or (epoch == epochs-1):\n",
        "            fixed_x = dataset[0][10].unsqueeze(0)\n",
        "            compare_x = compare(fixed_x.to(device))\n",
        "            save_image(compare_x.data.cpu(), '{}/images_vaeb/sample_image_epoch_{}.png'.format(base_url, epoch+1))\n",
        "\n",
        "        print('Epoch {}, loss {}'.format(epoch+1, recon_loss.item()))\n",
        "    # Store model. \n",
        "    torch.save(vae.state_dict(), 'vae.torch')\n",
        "\n",
        "    # Show final result. \n",
        "    img = mpimg.imread('{}/images_vaeb/sample_image_epoch_{}.png'.format(base_url, epochs))\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "main(50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:150: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, loss 223.96868896484375\n",
            "Epoch 2, loss 97.2720947265625\n",
            "Epoch 3, loss 53.61050796508789\n",
            "Epoch 4, loss 36.82554626464844\n",
            "Epoch 5, loss 32.377281188964844\n",
            "Epoch 6, loss 26.23065948486328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBu3MfoI3uqu",
        "colab_type": "text"
      },
      "source": [
        "## InfoVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIObI8uQql4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "    \n",
        "class Reshape(torch.nn.Module):\n",
        "    def __init__(self, outer_shape):\n",
        "        super(Reshape, self).__init__()\n",
        "        self.outer_shape = outer_shape\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), *self.outer_shape)\n",
        "\n",
        "# Encoder and decoder use the DC-GAN architecture\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = torch.nn.ModuleList([\n",
        "            torch.nn.Conv2d(3, 64, 4, 2, padding=1),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Conv2d(64, 128, 4, 2, padding=1),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            torch.nn.Linear(32768, 1024),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(1024, z_dim)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #print(\"Encoder\")\n",
        "        #print(x.size())\n",
        "        for layer in self.model:\n",
        "            x = layer(x)\n",
        "            #print(x.size())\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model = torch.nn.ModuleList([\n",
        "            torch.nn.Linear(z_dim, 1024),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 32768),\n",
        "            torch.nn.ReLU(),\n",
        "            Reshape((128,16,16,)),\n",
        "            torch.nn.ConvTranspose2d(128, 64, 4, 2, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(64, 3, 4, 2, padding=1),\n",
        "            torch.nn.Sigmoid()\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #print(\"Decoder\")\n",
        "        #print(x.size())\n",
        "        for layer in self.model:\n",
        "            x = layer(x)\n",
        "            #print(x.size())\n",
        "        return x\n",
        "\n",
        "def compute_kernel(x, y):\n",
        "    x_size = x.size(0)\n",
        "    y_size = y.size(0)\n",
        "    dim = x.size(1)\n",
        "    x = x.unsqueeze(1) # (x_size, 1, dim)\n",
        "    y = y.unsqueeze(0) # (1, y_size, dim)\n",
        "    tiled_x = x.expand(x_size, y_size, dim)\n",
        "    tiled_y = y.expand(x_size, y_size, dim)\n",
        "    kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)\n",
        "    return torch.exp(-kernel_input) # (x_size, y_size)\n",
        "\n",
        "def compute_mmd(x, y):\n",
        "    x_kernel = compute_kernel(x, x)\n",
        "    y_kernel = compute_kernel(y, y)\n",
        "    xy_kernel = compute_kernel(x, y)\n",
        "    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()\n",
        "    return mmd\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder(z_dim)\n",
        "        self.decoder = Decoder(z_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        return z, x_reconstructed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkNatkcuVX0Z",
        "colab_type": "code",
        "outputId": "8152b649-8333-4c40-fd1f-5348c2e11db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input, size=1024):\n",
        "        return input.view(input.size(0), size, 1, 1)\n",
        "\n",
        "%matplotlib inline\n",
        "z_dim = 4\n",
        "vae = Model(z_dim).to(device)#VAE(image_channels=3).to(device)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "print('Loaded model.')\n",
        "\n",
        "# https://github.com/napsternxg/pytorch-practice/blob/master/Pytorch%20-%20MMD%20VAE.ipynb\n",
        "def compare(x):\n",
        "    _, recon_x = vae(x)\n",
        "    return torch.cat([x, recon_x])\n",
        "\n",
        "\n",
        "def main(episodes):\n",
        "    dataset = torch.load('{}/file.pt'.format(base_url))[:10] / 255\n",
        "    batch_size = 256\n",
        "    dataloader = torch.utils.data.DataLoader(dataset.reshape(32*len(dataset), 3, 64, 64), batch_size=batch_size, shuffle=True)\n",
        "    epochs = 100\n",
        "    for epoch in range(epochs):\n",
        "        losses = [] \n",
        "        for i, b in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            x = b.to(device)\n",
        "            z, x_reconstructed = vae(x) \n",
        "\n",
        "            true_samples = torch.randn(200, z_dim, requires_grad=False).to(device)\n",
        "            mmd = compute_mmd(true_samples, z)\n",
        "            nll = (x_reconstructed - x).pow(2).mean()\n",
        "            loss = nll + mmd*2\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Store one image per epoch to showcase development of VAE.\n",
        "        print('Epoch {} loss {}'.format(epoch, loss.item()))\n",
        "        fixed_x = dataset[1][30].unsqueeze(0)\n",
        "        compare_x = compare(fixed_x.to(device))\n",
        "        save_image(compare_x.data.cpu(), '{}/images/sample_image_epoch_{}.png'.format(base_url, epoch+1))\n",
        "\n",
        "        print('Epoch {}, loss {}'.format(epoch+1, loss.item()))\n",
        "    # Store model. \n",
        "    torch.save(vae.state_dict(), '{}/vae.torch'.format(base_url))\n",
        "\n",
        "    # Show final result. \n",
        "    img = mpimg.imread('{}/images/sample_image_epoch_{}.png'.format(base_url, epochs))\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "main(50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model.\n",
            "Epoch 0 loss 3.2989015579223633\n",
            "(5, 64, 64, 1)\n",
            "(5, 64, 64, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8f3a00743830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-8f3a00743830>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_to_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8f3a00743830>\u001b[0m in \u001b[0;36mconvert_to_display\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 20480 into shape (64,2,2,64)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz63VjN8I1Wa",
        "colab_type": "text"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cHoILvYF8An",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input, size=1024):\n",
        "        return input.view(input.size(0), size, 1, 1)\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    # def __init__(self, image_channels=3, h_dim=1024, z_dim=64):\n",
        "    #     super(VAE, self).__init__()\n",
        "    #     self.encoder = nn.Sequential(\n",
        "    #         nn.Conv2d(image_channels, 4, kernel_size=4, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.Conv2d(4, 4, kernel_size=4, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.Conv2d(4, 4, kernel_size=4, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.Conv2d(4, 4, kernel_size=4, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         Flatten()\n",
        "    #     )\n",
        "    #     self.fc1 = nn.Linear(h_dim, z_dim)\n",
        "    #     self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "    #     self.fc3 = nn.Linear(z_dim, h_dim)\n",
        "        \n",
        "    #     self.decoder = nn.Sequential(\n",
        "    #         UnFlatten(),\n",
        "    #         nn.ConvTranspose2d(h_dim, 4, kernel_size=8, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.ConvTranspose2d(4, 4, kernel_size=7, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.ConvTranspose2d(4, 4, kernel_size=5, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.ConvTranspose2d(4, 4, kernel_size=6, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.ConvTranspose2d(4, 4, kernel_size=5, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.ConvTranspose2d(4, 4, kernel_size=3, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.ConvTranspose2d(4, image_channels, kernel_size=4, stride=2),\n",
        "    #         nn.Sigmoid(),\n",
        "    #     )\n",
        "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            Flatten()\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            UnFlatten(),\n",
        "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        # return torch.normal(mu, std)\n",
        "        esp = torch.randn(*mu.size()).to(device)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "    \n",
        "    def bottleneck(self, h):\n",
        "        mu, logvar = self.fc1(h), self.fc2(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z, mu, logvar = self.bottleneck(h)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.fc3(z)\n",
        "        z = self.decoder(z)\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, logvar = self.encode(x)\n",
        "        z = self.decode(z)\n",
        "        return z, mu, logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtzeTRt4FycZ",
        "colab_type": "code",
        "outputId": "daf4c30b-8ca1-46d4-8100-6a7e832f7281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import Adam\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "from random import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "device = torch.device('cuda')\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "vae = VAE(image_channels=3).to(device)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "print('Loaded model.')\n",
        "\n",
        "# https://github.com/sksq96/pytorch-vae/blob/master/vae-cnn.ipynb <-- source for code. \n",
        "def loss_fn(recon_x, x, mu, logvar):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) #torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD, BCE, KLD\n",
        "\n",
        "def compare(x):\n",
        "    recon_x, _, _ = vae(x)\n",
        "    return torch.cat([x, recon_x])\n",
        "\n",
        "def main(episodes):\n",
        "    dataset = torch.load('{}/file.pt'.format(base_url)) / 255\n",
        "    dataloader = torch.utils.data.DataLoader(dataset.reshape(32*len(dataset), 3, 64, 64), batch_size=batch_size, shuffle=True)\n",
        "    epochs = 100\n",
        "    for epoch in range(epochs):\n",
        "        losses = [] \n",
        "        for i, b in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            b = b.to(device)\n",
        "            recon_images, mu, logvar = vae(b)\n",
        "            loss, bce, kld = loss_fn(recon_images, b, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Store one image per epoch to showcase development of VAE.\n",
        "        if (epoch % 50 == 0) or (epoch == epochs-1):\n",
        "            fixed_x = dataset[0][10].unsqueeze(0)\n",
        "            compare_x = compare(fixed_x.to(device))\n",
        "            save_image(compare_x.data.cpu(), '{}/images/sample_image_epoch_{}.png'.format(base_url, epoch+1))\n",
        "\n",
        "        print('Epoch {}, loss {}'.format(epoch+1, loss.item()))\n",
        "    # Store model. \n",
        "    torch.save(vae.state_dict(), 'vae.torch')\n",
        "\n",
        "    # Show final result. \n",
        "    img = mpimg.imread('{}/images/sample_image_epoch_{}.png'.format(base_url, epochs))\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "main(50)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model.\n",
            "Epoch 1, loss 2230.425537109375\n",
            "Epoch 2, loss 780.3670654296875\n",
            "Epoch 3, loss 381.1385498046875\n",
            "Epoch 4, loss 259.9405822753906\n",
            "Epoch 5, loss 203.65121459960938\n",
            "Epoch 6, loss 181.8278350830078\n",
            "Epoch 7, loss 165.91921997070312\n",
            "Epoch 8, loss 162.89862060546875\n",
            "Epoch 9, loss 156.0592803955078\n",
            "Epoch 10, loss 152.92958068847656\n",
            "Epoch 11, loss 149.87413024902344\n",
            "Epoch 12, loss 142.90647888183594\n",
            "Epoch 13, loss 141.7327880859375\n",
            "Epoch 14, loss 142.99575805664062\n",
            "Epoch 15, loss 148.14590454101562\n",
            "Epoch 16, loss 137.66015625\n",
            "Epoch 17, loss 132.77113342285156\n",
            "Epoch 18, loss 140.1815185546875\n",
            "Epoch 19, loss 139.7767333984375\n",
            "Epoch 20, loss 146.4058837890625\n",
            "Epoch 21, loss 145.33950805664062\n",
            "Epoch 22, loss 168.84112548828125\n",
            "Epoch 23, loss 150.09544372558594\n",
            "Epoch 24, loss 156.44705200195312\n",
            "Epoch 25, loss 133.94271850585938\n",
            "Epoch 26, loss 135.8110809326172\n",
            "Epoch 27, loss 123.61334228515625\n",
            "Epoch 28, loss 136.90579223632812\n",
            "Epoch 29, loss 143.346435546875\n",
            "Epoch 30, loss 131.54376220703125\n",
            "Epoch 31, loss 136.76869201660156\n",
            "Epoch 32, loss 132.18829345703125\n",
            "Epoch 33, loss 131.85775756835938\n",
            "Epoch 34, loss 143.8628387451172\n",
            "Epoch 35, loss 129.8358154296875\n",
            "Epoch 36, loss 142.95260620117188\n",
            "Epoch 37, loss 135.39434814453125\n",
            "Epoch 38, loss 141.7767333984375\n",
            "Epoch 39, loss 138.8780517578125\n",
            "Epoch 40, loss 144.34408569335938\n",
            "Epoch 41, loss 138.5944366455078\n",
            "Epoch 42, loss 148.1184844970703\n",
            "Epoch 43, loss 134.33505249023438\n",
            "Epoch 44, loss 152.57179260253906\n",
            "Epoch 45, loss 146.27796936035156\n",
            "Epoch 46, loss 143.0469207763672\n",
            "Epoch 47, loss 136.89822387695312\n",
            "Epoch 48, loss 145.7882843017578\n",
            "Epoch 49, loss 135.92201232910156\n",
            "Epoch 50, loss 140.26904296875\n",
            "Epoch 51, loss 137.658447265625\n",
            "Epoch 52, loss 150.20504760742188\n",
            "Epoch 53, loss 137.46624755859375\n",
            "Epoch 54, loss 168.5581817626953\n",
            "Epoch 55, loss 132.66552734375\n",
            "Epoch 56, loss 148.95626831054688\n",
            "Epoch 57, loss 146.14700317382812\n",
            "Epoch 58, loss 140.417724609375\n",
            "Epoch 59, loss 138.58509826660156\n",
            "Epoch 60, loss 128.15170288085938\n",
            "Epoch 61, loss 129.15444946289062\n",
            "Epoch 62, loss 136.075927734375\n",
            "Epoch 63, loss 136.31163024902344\n",
            "Epoch 64, loss 133.39395141601562\n",
            "Epoch 65, loss 133.60765075683594\n",
            "Epoch 66, loss 126.80736541748047\n",
            "Epoch 67, loss 130.47708129882812\n",
            "Epoch 68, loss 130.57989501953125\n",
            "Epoch 69, loss 138.4420166015625\n",
            "Epoch 70, loss 142.47470092773438\n",
            "Epoch 71, loss 137.16221618652344\n",
            "Epoch 72, loss 141.79502868652344\n",
            "Epoch 73, loss 125.16726684570312\n",
            "Epoch 74, loss 136.7270050048828\n",
            "Epoch 75, loss 141.75039672851562\n",
            "Epoch 76, loss 169.05258178710938\n",
            "Epoch 77, loss 156.17861938476562\n",
            "Epoch 78, loss 140.80532836914062\n",
            "Epoch 79, loss 139.52394104003906\n",
            "Epoch 80, loss 137.625732421875\n",
            "Epoch 81, loss 135.46507263183594\n",
            "Epoch 82, loss 137.9095458984375\n",
            "Epoch 83, loss 140.2200164794922\n",
            "Epoch 84, loss 127.1063003540039\n",
            "Epoch 85, loss 137.4473876953125\n",
            "Epoch 86, loss 131.24916076660156\n",
            "Epoch 87, loss 133.26588439941406\n",
            "Epoch 88, loss 135.4325408935547\n",
            "Epoch 89, loss 138.4061737060547\n",
            "Epoch 90, loss 136.5509796142578\n",
            "Epoch 91, loss 135.77801513671875\n",
            "Epoch 92, loss 140.6480712890625\n",
            "Epoch 93, loss 137.43247985839844\n",
            "Epoch 94, loss 131.07815551757812\n",
            "Epoch 95, loss 138.78695678710938\n",
            "Epoch 96, loss 142.741943359375\n",
            "Epoch 97, loss 141.72573852539062\n",
            "Epoch 98, loss 136.1981658935547\n",
            "Epoch 99, loss 133.35372924804688\n",
            "Epoch 100, loss 130.7306671142578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a9e4a2894ac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-a9e4a2894ac6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Show final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/images/sample_image_epoch_{}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mimgplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content//gdrive/My Drive/projects/NIPS/neurosmash/images/sample_image_epoch_100.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mxhMQYm-oHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}