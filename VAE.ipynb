{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAE.ipynb","provenance":[],"collapsed_sections":["Rk6A_Pnt36c9","mBu3MfoI3uqu","hgBTzCLNnfGm","M0qklfnJ2uf6"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/JellePiepenbrock/neurosmash/blob/master/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"_91uizYrFu6h","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","from IPython.display import clear_output\n","import os, sys\n","\n","drive.mount('/content/gdrive/')\n","sys.path.append('/content/gdrive/My Drive/projects/entity_tagging/')\n","\n","base_url = '/content//gdrive/My Drive/projects/NIPS/'\n","\n","clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mz63VjN8I1Wa","colab_type":"text"},"source":["## VAE"]},{"cell_type":"code","metadata":{"id":"2cHoILvYF8An","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","class Flatten(nn.Module):\n","    def forward(self, input):\n","        return input.view(input.size(0), -1)\n","\n","class UnFlatten(nn.Module):\n","    def forward(self, input, size=32):\n","        return input.view(input.size(0), size, 1, 1)\n","\n","class VAE(nn.Module):\n","    def __init__(self, image_channels=3, h_dim=512, z_dim=32):\n","        super(VAE, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            Flatten()\n","        )\n","\n","        model_parameters = filter(lambda p: p.requires_grad, self.encoder.parameters())\n","        params = sum([np.prod(p.size()) for p in model_parameters])\n","        print('Trainable parameters encoder: ', params)\n","        \n","        self.fc1 = nn.Linear(h_dim, z_dim)\n","        self.fc2 = nn.Linear(h_dim, z_dim)\n","        self.fc3 = nn.Linear(z_dim, 32)\n","        \n","        self.decoder = nn.Sequential(\n","            UnFlatten(),\n","            nn.ConvTranspose2d(32, 32, kernel_size=5, stride=2),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 8, kernel_size=6, stride=2),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(8, image_channels, kernel_size=6, stride=2),\n","            nn.Sigmoid(),\n","        )\n","\n","        model_parameters = filter(lambda p: p.requires_grad, self.decoder.parameters())\n","        params = sum([np.prod(p.size()) for p in model_parameters])\n","        print('Trainable parameters decoder: ', params)\n","        \n","    def reparameterize(self, mu, logvar):\n","        std = logvar.mul(0.5).exp_()\n","        # return torch.normal(mu, std)\n","        esp = torch.randn(*mu.size()).to(device)\n","        z = mu + std * esp\n","        return z\n","    \n","    def bottleneck(self, h):\n","        mu, logvar = self.fc1(h), self.fc2(h)\n","        z = self.reparameterize(mu, logvar)\n","        return z, mu, logvar\n","\n","    def encode(self, x):\n","        h = self.encoder(x)\n","        z, mu, logvar = self.bottleneck(h)\n","        return z, mu, logvar\n","\n","    def decode(self, z):\n","        z = self.fc3(z)\n","        z = self.decoder(z)\n","        return z\n","\n","    def forward(self, x):\n","        z, mu, logvar = self.encode(x)\n","        z = self.decode(z)\n","        return z, mu, logvar\n","\n","\n","# vae = VAE(image_channels=3).to(device)\n","# optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIDWxKT1Y980","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from torch.optim import Adam\n","from PIL import Image\n","import torch.nn.functional as F\n","import numpy as np\n","import torch\n","from torch.autograd import Variable\n","from torch.distributions import Categorical\n","from random import randint\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","%matplotlib inline\n","\n","from torchvision.utils import save_image\n","from torchvision.transforms.functional import to_pil_image, to_grayscale, to_tensor\n","\n","device = torch.device('cuda')\n","torch.manual_seed(42)\n","batch_size = 32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8sS-5CK2Y7xP","colab_type":"code","colab":{}},"source":["dataset = torch.load('{}/neurosmash/training_data.pt'.format(base_url))\n","dataset_flattened = np.array([np.transpose(x.data.numpy(), (1,2,0)) for x in dataset.reshape(-1, 3, 64, 64)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NrvLSYflZf6o","colab_type":"code","colab":{}},"source":["backSub = cv2.createBackgroundSubtractorMOG2()\n","temp = np.uint8(dataset_flattened*255)[:10000]\n","    \n","for frame in temp:   \n","    frame = frame[0,:,:]\n","    fgMask = backSub.apply(frame)\n","\n","filters = np.repeat(np.array([np.array(backSub.apply(np.uint8(x_*255))) for x_ in dataset_flattened]).reshape(-1, 1, 64, 64), 3, axis=1)\n","dataset_flattened = np.transpose(dataset_flattened, (0, 3, 1, 2))\n","dataset_flattened = np.array([np.array([x, f]) for x,f in zip(dataset_flattened, filters)])\n","dataset_flattened = torch.from_numpy(dataset_flattened).float() #/ 255\n","dataloader = torch.utils.data.DataLoader(dataset_flattened, batch_size=batch_size, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtzeTRt4FycZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":727},"outputId":"216b9cbb-b2ea-4543-89d4-d79c4a7743a0","executionInfo":{"status":"error","timestamp":1577302602206,"user_tz":-60,"elapsed":56240,"user":{"displayName":"Mick van Hulst","photoUrl":"","userId":"08998285444095212659"}}},"source":["# https://github.com/sksq96/pytorch-vae/blob/master/vae-cnn.ipynb <-- source for code. \n","def loss_fn(recon_x, x, vae_weights, mu, logvar, use_weights):\n","    # Generate weights\n","\n","    # Weighted BCE\n","    # vae_weights = torch.ones(1, 3, 64, 64).to(device)\n","    # vae_weights[:,0,:,:] *= 0.001\n","    # vae_weights *= 0.99\n","    # vae_weights = vae_weights.repeat(recon_x.shape[0], 1, 1, 1)\n","    if use_weights:\n","        BCE = F.binary_cross_entropy(recon_x, x, weight=vae_weights, reduction='sum')\n","    else:\n","        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","\n","    # see Appendix B from VAE paper:\n","    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE + KLD, BCE, KLD\n","\n","def compare(x, f, vae):\n","    vae.eval()\n","    recon_x, _, _ = vae(x)\n","    vae.train()\n","    return torch.cat([x, recon_x, f])\n","\n","def max_rgb_filter(image):\n","\t(R, G, B) = cv2.split(image)\n"," \n","\t# find the maximum pixel intensity values for each\n","\t# (x, y)-coordinate,, then set all pixel values less\n","\t# than M to zero\n","\tM = np.maximum(np.maximum(R, G), B)\n","\tR[R < M] = 0\n","\tG[G < M] = 0\n","\tB[B < M] = 0\n"," \n","\t# merge the channels back together and return the image\n","\treturn cv2.merge([R, G, B])\n"," \n","def main(episodes):\n","    best_loss = 9999999\n","    epochs = 1000\n","    for use_weights in [0, 1]:\n","        print('===========================')\n","        print('Training new model with weights: {}'.format(use_weights))\n","        print('===========================')\n","        vae = VAE(image_channels=3).to(device)\n","        optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n","        vae.train()\n","        print('Loaded model.')\n","        for epoch in range(epochs):\n","            losses = [] \n","            batch_loss = 0\n","            \n","            for i, batch in enumerate(dataloader):\n","                optimizer.zero_grad()\n","\n","                f = batch[:,1,:,:,:].squeeze().to(device)\n","                f[f > 0] = 5\n","                f[f == 0] = 0.5\n","\n","                b = batch[:,0,:,:,:].squeeze().to(device)\n","                recon_images, mu, logvar = vae(b)\n","                \n","                loss, bce, kld = loss_fn(recon_images, b, f, mu, logvar, use_weights)\n","                loss.backward()\n","                optimizer.step()\n","                batch_loss += loss.item()\n","\n","            batch_loss /= len(dataloader)\n","            if batch_loss < best_loss:\n","                print('Storing weights', batch_loss, best_loss)\n","                best_loss = batch_loss\n","                torch.save(vae.state_dict(), '{}/neurosmash/vae_v3_weights_{}.torch'.format(base_url, use_weights))\n","\n","            if epoch == 200:\n","                print('reducing lr')\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = 1e-4\n","            elif epoch == 400:\n","                print('reducing lr')\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = 1e-5\n","            \n","            print('Epoch {}, loss {}'.format(epoch+1, batch_loss / batch_size))\n","\n","            if epoch % 5 == 0:\n","                # Store model. \n","                idx = randint(0, len(b)-1)           \n","                fixed_x = b[idx].unsqueeze(0)\n","                fixed_filter = batch[:,1,:,:,:].squeeze()[idx].unsqueeze(0)\n","                compare_x = compare(fixed_x.to(device), fixed_filter.to(device), vae)\n","                \n","                # print(torch.stack(, compare_x).shape)\n","                save_image(compare_x.data.cpu(), '{}/img_vae_{}/sample_image_epoch_{}.png'.format(base_url, use_weights, epoch))\n","\n","                # Show final result. \n","                img = mpimg.imread('{}/img_vae_{}/sample_image_epoch_{}.png'.format(base_url, use_weights, epoch))\n","                imgplot = plt.imshow(img.squeeze())\n","                plt.show()\n","main(50)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["===========================\n","Training new model with weights: 0\n","===========================\n","Trainable parameters encoder:  231200\n","Trainable parameters decoder:  43931\n","Loaded model.\n","Storing weights 10857.846371484375 9999999\n","Epoch 1, loss 339.3076991088867\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXAAAACUCAYAAACKu3IfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Storing weights 4587.991545703125 10857.846371484375\n","Epoch 2, loss 143.37473580322265\n","Storing weights 4578.132530078125 4587.991545703125\n","Epoch 3, loss 143.0666415649414\n","Storing weights 4575.337532421875 4578.132530078125\n","Epoch 4, loss 142.9792978881836\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-408f2b79bb7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mimgplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-408f2b79bb7c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"1CFoZBVMiRgK","colab_type":"code","colab":{}},"source":["#TODO: Load model.\n","\n","print('Encoding training data.')\n","#TODO Store all processed dps.\n","res = None\n","vae.eval()\n","for batch in dataset:\n","    batch = batch.to(device)\n","    z, mu, logvar = vae.encode(batch)\n","    z = z.unsqueeze(0)\n","    if res is None:\n","        res = z\n","    else:\n","        res = torch.cat((res, z))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rk6A_Pnt36c9","colab_type":"text"},"source":["## Beta VAE"]},{"cell_type":"code","metadata":{"id":"K1bTDW4g35_t","colab_type":"code","colab":{}},"source":["\"\"\"model.py\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","#import torch.nn.functional as F\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","\n","\n","def reparametrize(mu, logvar):\n","    std = logvar.div(2).exp()\n","    eps = Variable(std.data.new(std.size()).normal_())\n","    return mu + std*eps\n","\n","\n","class View(nn.Module):\n","    def __init__(self, size):\n","        super(View, self).__init__()\n","        self.size = size\n","\n","    def forward(self, tensor):\n","        return tensor.view(self.size)\n","\n","\n","class BetaVAE_H(nn.Module):\n","    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017).\"\"\"\n","\n","    def __init__(self, z_dim=10, nc=3):\n","        super(BetaVAE_H, self).__init__()\n","        self.z_dim = z_dim\n","        self.nc = nc\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n","            nn.ReLU(True),\n","            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n","            nn.ReLU(True),\n","            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1\n","            nn.ReLU(True),\n","            View((-1, 256*1*1)),                 # B, 256\n","            nn.Linear(256, z_dim*2),             # B, z_dim*2\n","        )\n","\n","        model_parameters = filter(lambda p: p.requires_grad, self.encoder.parameters())\n","        params = sum([np.prod(p.size()) for p in model_parameters])\n","        print('Trainable parameters encoder: ', params)\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim, 64),               # B, 256\n","            View((-1, 64, 1, 1)),               # B, 256,  1,  1\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 32, 4),      # B,  64,  4,  4\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(32, 16, 4, 2, 1), # B,  64,  8,  8\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(16, 8, 4, 2, 1), # B,  32, 16, 16\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(8, 8, 4, 2, 1), # B,  32, 32, 32\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(8, nc, 4, 2, 1),  # B, nc, 64, 64\n","        )\n","\n","        model_parameters = filter(lambda p: p.requires_grad, self.decoder.parameters())\n","        params = sum([np.prod(p.size()) for p in model_parameters])\n","        print('Trainable parameters encoder: ', params)\n","\n","        self.weight_init()\n","\n","    def weight_init(self):\n","        for block in self._modules:\n","            for m in self._modules[block]:\n","                kaiming_init(m)\n","\n","    def forward(self, x):\n","        distributions = self._encode(x)\n","        mu = distributions[:, :self.z_dim]\n","        logvar = distributions[:, self.z_dim:]\n","        z = reparametrize(mu, logvar)\n","        x_recon = self._decode(z)\n","\n","        return x_recon, mu, logvar\n","\n","    def _encode(self, x):\n","        return self.encoder(x)\n","\n","    def _decode(self, z):\n","        return self.decoder(z)\n","\n","\n","def kaiming_init(m):\n","    if isinstance(m, (nn.Linear, nn.Conv2d)):\n","        init.kaiming_normal(m.weight)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)\n","    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n","        m.weight.data.fill_(1)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)\n","\n","\n","def normal_init(m, mean, std):\n","    if isinstance(m, (nn.Linear, nn.Conv2d)):\n","        m.weight.data.normal_(mean, std)\n","        if m.bias.data is not None:\n","            m.bias.data.zero_()\n","    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n","        m.weight.data.fill_(1)\n","        if m.bias.data is not None:\n","            m.bias.data.zero_()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhOb06em4TuH","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from torch.optim import Adam\n","from PIL import Image\n","import torch.nn.functional as F\n","import numpy as np\n","import torch\n","from torch.autograd import Variable\n","from torch.distributions import Categorical\n","from random import randint\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from skimage import io, transform\n","import cv2\n","\n","%matplotlib inline\n","\n","from torchvision.utils import save_image\n","device = torch.device('cuda')\n","\n","batch_size = 64\n","\n","objective = 'H'\n","beta = 2\n","lr =  1e-4\n","z_dim =  32\n","max_iter =  1.5e6\n","decoder_dist = 'gaussian'\n","\n","vae = BetaVAE_H(nc=3, z_dim=z_dim)\n","vae = vae.to(device)\n","optimizer = torch.optim.Adam(vae.parameters(), lr=lr, betas=(0.9, 0.999))\n","print('Loaded model.')\n","\n","# https://github.com/1Konny/Beta-VAE/blob/master/solver.py \n","\n","def mse_loss(input, target):\n","    return torch.sum((input - target) ** 2)\n","\n","def weighted_mse_loss(input, target, weight):\n","    return torch.sum(weight * (input - target) ** 2)\n","\n","def reconstruction_loss(x, x_recon, weights, distribution):\n","    batch_size = x.size(0)\n","    assert batch_size != 0\n","\n","    if distribution == 'bernoulli':\n","        recon_loss = F.binary_cross_entropy_with_logits(x_recon, x, size_average=False).div(batch_size)\n","    elif distribution == 'gaussian':\n","        # x_recon = F.sigmoid(x_recon)\n","        recon_loss = weighted_mse_loss(x_recon, x, weight=weights).sum() #F.mse_loss(x_recon, x, weights=weights, reduction='sum').div(batch_size)#, size_average=False).div(batch_size)\n","    else:\n","        recon_loss = None\n","\n","    return recon_loss\n","\n","\n","def kl_divergence(mu, logvar):\n","    batch_size = mu.size(0)\n","    assert batch_size != 0\n","    if mu.data.ndimension() == 4:\n","        mu = mu.view(mu.size(0), mu.size(1))\n","    if logvar.data.ndimension() == 4:\n","        logvar = logvar.view(logvar.size(0), logvar.size(1))\n","\n","    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n","    total_kld = klds.sum(1).mean(0, True)\n","    dimension_wise_kld = klds.mean(0)\n","    mean_kld = klds.mean(1).mean(0, True)\n","\n","    return total_kld, dimension_wise_kld, mean_kld\n","\n","\n","def compare(x):\n","    recon_x, _, _ = vae(x)\n","    return torch.cat([x, recon_x])\n","\n","def max_rgb_filter(image):\n","\t# split the image into its BGR components\n","\t(R, G, B) = cv2.split(image)\n"," \n","\t# find the maximum pixel intensity values for each\n","\t# (x, y)-coordinate,, then set all pixel values less\n","\t# than M to zero\n","\tM = np.maximum(np.maximum(R, G), B)\n","\tR[R < M] = 0\n","\tG[G < M] = 0\n","\tB[B < M] = 0\n"," \n","\t# merge the channels back together and return the image\n","\treturn cv2.merge([R, G, B])\n","def main(episodes):\n","    dataset = None\n","    for i in range(1):\n","        if dataset is None:\n","            dataset = torch.load('{}/data/training_data_{}.pt'.format(base_url, i)) / 255\n","        else:\n","            dataset = torch.cat((dataset, torch.load('{}/data/training_data_{}.pt'.format(base_url, i)) / 255))\n","    \n","    test_dataset = torch.load('{}/data/training_data_{}.pt'.format(base_url, i+1)) / 255\n","    dataset = np.array([np.transpose(x.data.numpy(), (1,2,0)) for x in dataset])  \n","\n","    if mode == 'ext':\n","        dataset = [max_rgb_filter(np.uint8(x*255)) for x in dataset]\n","        dataset = np.transpose(dataset, (0, 3, 1, 2))\n","        dataset[:,0,:,:] *= 0\n","    elif mode == 'canny':\n","        filters = [cv2.Canny(np.uint8(x_*255*255), 100, 200) for x_ in dataset]\n","        dataset = np.array([cv2.bitwise_and(x_, x_, mask=f) for x_, f in zip(dataset, filters)])\n","        dataset = np.transpose(dataset, (0, 3, 1, 2))\n","    elif mode == 'filter':\n","        filters = np.repeat(np.array([np.array(cv2.Canny(np.uint8(x_*255*255), 100, 200)) for x_ in dataset]).reshape(-1, 1, 64, 64), 3, axis=1)\n","        dataset = np.transpose(dataset, (0, 3, 1, 2))\n","        dataset = np.array([np.array([x, f]) for x,f in zip(dataset, filters)])\n","\n","    dataset = torch.from_numpy(dataset).float() #/ 255\n","\n","    # dataset = dataset[:100]\n","    # # Divide by 255\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    epochs = 125\n","    print(dataset.shape, test_dataset.shape)\n","    for epoch in range(epochs):\n","        losses = [] \n","        for i, batch in enumerate(dataloader):\n","            optimizer.zero_grad()\n","\n","            f = (batch[:,1,:,:,:].squeeze()).to(device)\n","            f[f == 0] = 0.05\n","            f[f == 255] = 10\n","\n","            b = batch[:,0,:,:,:].squeeze().to(device)\n","\n","            x_recon, mu, logvar = vae(b)\n","            recon_loss = reconstruction_loss(b, x_recon, f, decoder_dist)\n","            total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n","\n","            loss = recon_loss + beta*total_kld\n","            # elif objective == 'B':\n","            #     C = torch.clamp(self.C_max/self.C_stop_iter*self.global_iter, 0, self.C_max.data[0])\n","            #     beta_vae_loss = recon_loss + self.gamma*(total_kld-C).abs()\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        if epoch == 100:\n","            print('reducing lr')\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = 1e-4\n","        print('Epoch {}, loss {}'.format(epoch+1, loss.item() / batch_size))\n","        if epoch % 5 == 0:\n","            fixed_x = b[randint(0, len(b)-1)].unsqueeze(0)\n","            compare_x = compare(fixed_x.to(device))\n","            \n","            # print(torch.stack(, compare_x).shape)\n","            save_image(compare_x.data.cpu(), '{}/neurosmash/images/sample_image_epoch_{}.png'.format(base_url, epoch))\n","\n","            # Show final result. \n","            img = mpimg.imread('{}/neurosmash/images/sample_image_epoch_{}.png'.format(base_url, epoch))\n","            imgplot = plt.imshow(img.squeeze())\n","            plt.show()\n","    \n","    print('Predicting test set:')\n","    for i, batch in enumerate(test_dataloader):\n","        if i > 10:\n","            break\n","        b = b.to(device)\n","        recon_images, mu, logvar = vae(b)\n","\n","        fixed_x = b[randint(0, len(b)-1)].unsqueeze(0)\n","        compare_x = compare(fixed_x.to(device))\n","        \n","        save_image(compare_x.data.cpu(), '{}/neurosmash/images/sample_image_test_epoch_{}.png'.format(base_url, epoch))\n","\n","        # Show final result. \n","        img = mpimg.imread('{}/neurosmash/images/sample_image_test_epoch_{}.png'.format(base_url, epoch))\n","        imgplot = plt.imshow(img.squeeze())\n","        plt.show()\n","\n","    # Store model. \n","    torch.save(vae.state_dict(), 'b_vae.torch')\n","main(50)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mBu3MfoI3uqu","colab_type":"text"},"source":["## InfoVAE"]},{"cell_type":"code","metadata":{"id":"AIObI8uQql4s","colab_type":"code","colab":{}},"source":["class Flatten(torch.nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","    \n","class Reshape(torch.nn.Module):\n","    def __init__(self, outer_shape):\n","        super(Reshape, self).__init__()\n","        self.outer_shape = outer_shape\n","    def forward(self, x):\n","        return x.view(x.size(0), *self.outer_shape)\n","\n","# Encoder and decoder use the DC-GAN architecture\n","class Encoder(torch.nn.Module):\n","    def __init__(self, z_dim):\n","        super(Encoder, self).__init__()\n","        self.model = torch.nn.ModuleList([\n","            torch.nn.Conv2d(3, 64, 4, 2, padding=1),\n","            torch.nn.LeakyReLU(),\n","            torch.nn.Conv2d(64, 128, 4, 2, padding=1),\n","            torch.nn.LeakyReLU(),\n","            Flatten(),\n","            torch.nn.Linear(32768, 1024),\n","            torch.nn.LeakyReLU(),\n","            torch.nn.Linear(1024, z_dim)\n","        ])\n","        \n","    def forward(self, x):\n","        #print(\"Encoder\")\n","        #print(x.size())\n","        for layer in self.model:\n","            x = layer(x)\n","            #print(x.size())\n","        return x\n","    \n","    \n","class Decoder(torch.nn.Module):\n","    def __init__(self, z_dim):\n","        super(Decoder, self).__init__()\n","        self.model = torch.nn.ModuleList([\n","            torch.nn.Linear(z_dim, 1024),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(1024, 32768),\n","            torch.nn.ReLU(),\n","            Reshape((128,16,16,)),\n","            torch.nn.ConvTranspose2d(128, 64, 4, 2, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.ConvTranspose2d(64, 3, 4, 2, padding=1),\n","            torch.nn.Sigmoid()\n","        ])\n","        \n","    def forward(self, x):\n","        #print(\"Decoder\")\n","        #print(x.size())\n","        for layer in self.model:\n","            x = layer(x)\n","            #print(x.size())\n","        return x\n","\n","def compute_kernel(x, y):\n","    x_size = x.size(0)\n","    y_size = y.size(0)\n","    dim = x.size(1)\n","    x = x.unsqueeze(1) # (x_size, 1, dim)\n","    y = y.unsqueeze(0) # (1, y_size, dim)\n","    tiled_x = x.expand(x_size, y_size, dim)\n","    tiled_y = y.expand(x_size, y_size, dim)\n","    kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)\n","    return torch.exp(-kernel_input) # (x_size, y_size)\n","\n","def compute_mmd(x, y):\n","    x_kernel = compute_kernel(x, x)\n","    y_kernel = compute_kernel(y, y)\n","    xy_kernel = compute_kernel(x, y)\n","    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()\n","    return mmd\n","\n","class Model(torch.nn.Module):\n","    def __init__(self, z_dim):\n","        super(Model, self).__init__()\n","        self.encoder = Encoder(z_dim)\n","        self.decoder = Decoder(z_dim)\n","        \n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_reconstructed = self.decoder(z)\n","        return z, x_reconstructed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkNatkcuVX0Z","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import math\n","import matplotlib.pyplot as plt\n","\n","\n","class Flatten(nn.Module):\n","    def forward(self, input):\n","        return input.view(input.size(0), -1)\n","\n","class UnFlatten(nn.Module):\n","    def forward(self, input, size=1024):\n","        return input.view(input.size(0), size, 1, 1)\n","\n","%matplotlib inline\n","z_dim = 4\n","vae = Model(z_dim).to(device)#VAE(image_channels=3).to(device)\n","optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n","print('Loaded model.')\n","\n","# https://github.com/napsternxg/pytorch-practice/blob/master/Pytorch%20-%20MMD%20VAE.ipynb\n","def compare(x):\n","    _, recon_x = vae(x)\n","    return torch.cat([x, recon_x])\n","\n","\n","def main(episodes):\n","    dataset = torch.load('{}/data/training_data_0.pt'.format(base_url)) #/ 255\n","\n","    # Grayscale convertion.\n","    # dataset = [to_pil_image(x_) for x_ in dataset]\n","    # dataset = [to_grayscale(x_, 1) for x_ in dataset]\n","    # dataset = torch.stack([to_tensor(x_) for x_ in dataset])\n","\n","    dataset = np.array([np.transpose(x.data.numpy(), (1,2,0)) for x in dataset])\n","\n","    filters = [cv2.Canny(np.uint8(x_*255), 100, 200) for x_ in dataset]\n","    dataset = np.array([cv2.bitwise_and(x_, x_, mask=f) for x_, f in zip(dataset, filters)])\n","    dataset = np.transpose(dataset, (0, 3, 1, 2))\n","\n","\n","    # Divide by 255\n","    dataset = torch.from_numpy(dataset) / 255\n","    dataloader = torch.utils.data.DataLoader(dataset[:100], batch_size=batch_size, shuffle=True)\n","    epochs = 10000\n","    for epoch in range(epochs):\n","        losses = [] \n","        for i, b in enumerate(dataloader):\n","            optimizer.zero_grad()\n","            x = b.to(device)\n","            z, x_reconstructed = vae(x) \n","\n","            true_samples = torch.randn(200, z_dim, requires_grad=False).to(device)\n","            mmd = compute_mmd(true_samples, z)\n","            nll = (x_reconstructed - x).pow(2).sum()\n","            loss = nll + mmd*2\n","            loss.backward()\n","            optimizer.step()\n","        if (epoch % 500 == 0) or (epoch == epochs-1):\n","            # Store one image per epoch to showcase development of VAE.\n","            print('Epoch {} loss {}'.format(epoch, loss.item()))\n","            fixed_x = dataset[1].unsqueeze(0)\n","            compare_x = compare(fixed_x.to(device))\n","            save_image(compare_x.data.cpu(), '{}/neurosmash/images_infovae/sample_image_epoch_{}.png'.format(base_url, epoch+1))\n","\n","        print('Epoch {}, loss {}'.format(epoch+1, loss.item()))\n","    # Store model. \n","    torch.save(vae.state_dict(), '{}/vae.torch'.format(base_url))\n","\n","    # Show final result. \n","    img = mpimg.imread('{}/neurosmash/images_infovae/sample_image_epoch_{}.png'.format(base_url, epochs))\n","    imgplot = plt.imshow(img)\n","    plt.show()\n","\n","main(50)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgBTzCLNnfGm","colab_type":"text"},"source":["## VAE-GAN <-- wtf is even happening here.."]},{"cell_type":"code","metadata":{"id":"5Udmef3Ongqy","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy\n","from torchvision.utils import save_image\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from torch.optim import Adam\n","from PIL import Image\n","import torch.nn.functional as F\n","import numpy as np\n","import torch\n","from torch.autograd import Variable\n","from torch.distributions import Categorical\n","from random import randint\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from skimage import io, transform\n","\n","\n","# encoder block (used in encoder and discriminator)\n","class EncoderBlock(nn.Module):\n","    def __init__(self, channel_in, channel_out):\n","        super(EncoderBlock, self).__init__()\n","        # convolution to halve the dimensions\n","        self.conv = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=5, padding=2, stride=2,\n","                              bias=False)\n","        self.bn = nn.BatchNorm2d(num_features=channel_out, momentum=0.9)\n","\n","    def forward(self, ten, out=False,t = False):\n","        # here we want to be able to take an intermediate output for reconstruction error\n","        if out:\n","            ten = self.conv(ten)\n","            ten_out = ten\n","            ten = self.bn(ten)\n","            ten = F.relu(ten, False)\n","            return ten, ten_out\n","        else:\n","            ten = self.conv(ten)\n","            ten = self.bn(ten)\n","            ten = F.relu(ten, True)\n","            return ten\n","\n","\n","# decoder block (used in the decoder)\n","class DecoderBlock(nn.Module):\n","    def __init__(self, channel_in, channel_out):\n","        super(DecoderBlock, self).__init__()\n","        # transpose convolution to double the dimensions\n","        self.conv = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=5, padding=2, stride=2, output_padding=1,\n","                                       bias=False)\n","        self.bn = nn.BatchNorm2d(channel_out, momentum=0.9)\n","\n","    def forward(self, ten):\n","        ten = self.conv(ten)\n","        ten = self.bn(ten)\n","        ten = F.relu(ten, True)\n","        return ten\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, channel_in=3, z_size=128):\n","        super(Encoder, self).__init__()\n","        self.size = channel_in\n","        layers_list = []\n","        # the first time 3->64, for every other double the channel size\n","        for i in range(3):\n","            if i == 0:\n","                layers_list.append(EncoderBlock(channel_in=self.size, channel_out=64))\n","                self.size = 64\n","            else:\n","                layers_list.append(EncoderBlock(channel_in=self.size, channel_out=self.size * 2))\n","                self.size *= 2\n","\n","        # final shape Bx256x8x8\n","        self.conv = nn.Sequential(*layers_list)\n","        self.fc = nn.Sequential(nn.Linear(in_features=8 * 8 * self.size, out_features=1024, bias=False),\n","                                nn.BatchNorm1d(num_features=1024,momentum=0.9),\n","                                nn.ReLU(True))\n","        # two linear to get the mu vector and the diagonal of the log_variance\n","        self.l_mu = nn.Linear(in_features=1024, out_features=z_size)\n","        self.l_var = nn.Linear(in_features=1024, out_features=z_size)\n","\n","    def forward(self, ten):\n","        ten = self.conv(ten)\n","        ten = ten.view(len(ten), -1)\n","        ten = self.fc(ten)\n","        mu = self.l_mu(ten)\n","        logvar = self.l_var(ten)\n","        return mu, logvar\n","\n","    def __call__(self, *args, **kwargs):\n","        return super(Encoder, self).__call__(*args, **kwargs)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, z_size, size):\n","        super(Decoder, self).__init__()\n","        # start from B*z_size\n","        self.fc = nn.Sequential(nn.Linear(in_features=z_size, out_features=8 * 8 * size, bias=False),\n","                                nn.BatchNorm1d(num_features=8 * 8 * size,momentum=0.9),\n","                                nn.ReLU(True))\n","        self.size = size\n","        layers_list = []\n","        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size))\n","        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size//2))\n","        self.size = self.size//2\n","        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size//4))\n","        self.size = self.size//4\n","        # final conv to get 3 channels and tanh layer\n","        layers_list.append(nn.Sequential(\n","            nn.Conv2d(in_channels=self.size, out_channels=3, kernel_size=5, stride=1, padding=2),\n","            nn.Tanh()\n","        ))\n","\n","        self.conv = nn.Sequential(*layers_list)\n","\n","    def forward(self, ten):\n","\n","        ten = self.fc(ten)\n","        ten = ten.view(len(ten), -1, 8, 8)\n","        ten = self.conv(ten)\n","        return ten\n","\n","    def __call__(self, *args, **kwargs):\n","        return super(Decoder, self).__call__(*args, **kwargs)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, channel_in=3,recon_level=3):\n","        super(Discriminator, self).__init__()\n","        self.size = channel_in\n","        self.recon_levl = recon_level\n","        # module list because we need need to extract an intermediate output\n","        self.conv = nn.ModuleList()\n","        self.conv.append(nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(inplace=True)))\n","        self.size = 32\n","        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=128))\n","        self.size = 128\n","        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=256))\n","        self.size = 256\n","        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=256))\n","        # final fc to get the score (real or fake)\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_features=8 * 8 * self.size, out_features=512, bias=False),\n","            nn.BatchNorm1d(num_features=512,momentum=0.9),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(in_features=512, out_features=1),\n","\n","        )\n","\n","    def forward(self, ten,other_ten,mode='REC'):\n","        if mode == \"REC\":\n","            ten = torch.cat((ten, other_ten), 0)\n","            for i, lay in enumerate(self.conv):\n","                # we take the 9th layer as one of the outputs\n","                if i == self.recon_levl:\n","                    ten, layer_ten = lay(ten, True)\n","                    # we need the layer representations just for the original and reconstructed,\n","                    # flatten, because it's a convolutional shape\n","                    layer_ten = layer_ten.view(len(layer_ten), -1)\n","                    return layer_ten\n","                else:\n","                    ten = lay(ten)\n","        else:\n","            ten = torch.cat((ten, other_ten), 0)\n","            for i, lay in enumerate(self.conv):\n","                    ten = lay(ten)\n","\n","            ten = ten.view(len(ten), -1)\n","            ten = self.fc(ten)\n","            return F.sigmoid(ten)\n","\n","\n","    def __call__(self, *args, **kwargs):\n","        return super(Discriminator, self).__call__(*args, **kwargs)\n","\n","\n","class VaeGan(nn.Module):\n","    def __init__(self,z_size=128,recon_level=3):\n","        super(VaeGan, self).__init__()\n","        # latent space size\n","        self.z_size = z_size\n","        self.encoder = Encoder(z_size=self.z_size)\n","        self.decoder = Decoder(z_size=self.z_size, size=self.encoder.size)\n","        self.discriminator = Discriminator(channel_in=3,recon_level=recon_level)\n","        # self-defined function to init the parameters\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        # just explore the network, find every weight and bias matrix and fill it\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n","                if hasattr(m, \"weight\") and m.weight is not None and m.weight.requires_grad:\n","                    #init as original implementation\n","                    scale = 1.0/numpy.sqrt(numpy.prod(m.weight.shape[1:]))\n","                    scale /=numpy.sqrt(3)\n","                    #nn.init.xavier_normal(m.weight,1)\n","                    #nn.init.constant(m.weight,0.005)\n","                    nn.init.uniform(m.weight,-scale,scale)\n","                if hasattr(m, \"bias\") and m.bias is not None and m.bias.requires_grad:\n","                    nn.init.constant(m.bias, 0.0)\n","\n","    def forward(self, ten, gen_size=10):\n","        if self.training:\n","            # save the original images\n","            ten_original = ten\n","            # encode\n","            mus, log_variances = self.encoder(ten)\n","            # we need the true variances, not the log one\n","            variances = torch.exp(log_variances * 0.5)\n","            # sample from a gaussian\n","\n","            ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=True)\n","            # shift and scale using the means and variances\n","\n","            ten = ten_from_normal * variances + mus\n","            # decode the tensor\n","            ten = self.decoder(ten)\n","            # discriminator for reconstruction\n","            ten_layer = self.discriminator(ten, ten_original, \"REC\")\n","            # decoder for samples\n","\n","            ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=True)\n","\n","            ten = self.decoder(ten_from_normal)\n","            ten_class = self.discriminator(ten_original, ten, \"GAN\")\n","            return ten, ten_class, ten_layer, mus, log_variances\n","        else:\n","            if ten is None:\n","                # just sample and decode\n","\n","                ten = Variable(torch.randn(gen_size, self.z_size).cuda(), requires_grad=False)\n","                ten = self.decoder(ten)\n","            else:\n","                mus, log_variances = self.encoder(ten)\n","                # we need the true variances, not the log one\n","                variances = torch.exp(log_variances * 0.5)\n","                # sample from a gaussian\n","\n","                ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=False)\n","                # shift and scale using the means and variances\n","                ten = ten_from_normal * variances + mus\n","                # decode the tensor\n","                ten = self.decoder(ten)\n","            return ten\n","\n","\n","\n","    def __call__(self, *args, **kwargs):\n","        return super(VaeGan, self).__call__(*args, **kwargs)\n","\n","    @staticmethod\n","    def loss(ten_original, ten_predict, layer_original, layer_predicted, labels_original,\n","             labels_sampled, mus, variances):\n","        \"\"\"\n","        :param ten_original: original images\n","        :param ten_predict:  predicted images (output of the decoder)\n","        :param layer_original:  intermediate layer for original (intermediate output of the discriminator)\n","        :param layer_predicted: intermediate layer for reconstructed (intermediate output of the discriminator)\n","        :param labels_original: labels for original (output of the discriminator)\n","        :param labels_predicted: labels for reconstructed (output of the discriminator)\n","        :param labels_sampled: labels for sampled from gaussian (0,1) (output of the discriminator)\n","        :param mus: tensor of means\n","        :param variances: tensor of diagonals of log_variances\n","        :return:\n","        \"\"\"\n","\n","        # reconstruction error, not used for the loss but useful to evaluate quality\n","        nle = 0.5*(ten_original.view(len(ten_original), -1) - ten_predict.view(len(ten_predict), -1)) ** 2\n","        # kl-divergence\n","        kl = -0.5 * torch.sum(-variances.exp() - torch.pow(mus,2) + variances + 1, 1)\n","        # mse between intermediate layers\n","        mse = torch.sum(0.5*(layer_original - layer_predicted) ** 2, 1)\n","        # bce for decoder and discriminator for original,sampled and reconstructed\n","        # the only excluded is the bce_gen_original\n","\n","        bce_dis_original = -torch.log(labels_original + 1e-3)\n","        bce_dis_sampled = -torch.log(1 - labels_sampled + 1e-3)\n","\n","        bce_gen_original = -torch.log(1-labels_original + 1e-3)\n","        bce_gen_sampled = -torch.log(labels_sampled + 1e-3)\n","        '''\n","        \n","        bce_gen_predicted = nn.BCEWithLogitsLoss(size_average=False)(labels_predicted,\n","                                         Variable(torch.ones_like(labels_predicted.data).cuda(), requires_grad=False))\n","        bce_gen_sampled = nn.BCEWithLogitsLoss(size_average=False)(labels_sampled,\n","                                       Variable(torch.ones_like(labels_sampled.data).cuda(), requires_grad=False))\n","        bce_dis_original = nn.BCEWithLogitsLoss(size_average=False)(labels_original,\n","                                        Variable(torch.ones_like(labels_original.data).cuda(), requires_grad=False))\n","        bce_dis_predicted = nn.BCEWithLogitsLoss(size_average=False)(labels_predicted,\n","                                         Variable(torch.zeros_like(labels_predicted.data).cuda(), requires_grad=False))\n","        bce_dis_sampled = nn.BCEWithLogitsLoss(size_average=False)(labels_sampled,\n","                                       Variable(torch.zeros_like(labels_sampled.data).cuda(), requires_grad=False))\n","        '''\n","        return nle, kl, mse, bce_dis_original, bce_dis_sampled,bce_gen_original,bce_gen_sampled"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6UNJjPZo9Te","colab_type":"code","colab":{}},"source":["# just a class to store a rolling average\n","# useful to log to TB\n","class RollingMeasure(object):\n","    def __init__(self):\n","        self.measure = 0.0\n","        self.iter = 0\n","\n","    def __call__(self, measure):\n","        # passo nuovo valore e ottengo average\n","        # se first call inizializzo\n","        if self.iter == 0:\n","            self.measure = measure\n","        else:\n","            self.measure = (1.0 / self.iter * measure) + (1 - 1.0 / self.iter) * self.measure\n","        self.iter += 1\n","        return self.measure"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXQxhqV_pK8W","colab_type":"code","colab":{}},"source":["!pip install tensorboardX"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3G91unznmx7","colab_type":"code","colab":{}},"source":["import torch\n","import numpy\n","numpy.random.seed(8)\n","torch.manual_seed(8)\n","torch.cuda.manual_seed(8)\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","from tensorboardX import SummaryWriter\n","from torch.optim import RMSprop,Adam,SGD\n","from torch.optim.lr_scheduler import ExponentialLR,MultiStepLR\n","import progressbar\n","from torchvision.utils import make_grid\n","\n","\n","if __name__ == \"__main__\":\n","\n","    train_folder = '{}/data/training_data_0.pt'.format(base_url)\n","    test_folder = '{}/data/training_data_1.pt'.format(base_url)\n","    z_size = 128#args.z_size\n","    recon_level = 3#args.recon_level\n","    decay_mse = 1#args.decay_mse\n","    decay_margin = 1#args.decay_margin\n","    n_epochs = 12#args.n_epochs\n","    lambda_mse = 1e-6#args.lambda_mse\n","    lr = 3e-4#args.lr\n","    decay_lr = 0.75#args.decay_lr\n","    decay_equilibrium = 1#args.decay_equilibrium\n","    slurm = False #args.slurm\n","\n","    writer = SummaryWriter(comment=\"_CELEBA_NEW_DATA_STOCK_GAN\")\n","    net = VaeGan(z_size=z_size,recon_level=recon_level).cuda()\n","\n","    # train = torch.load(train_folder)\n","\n","    # DATASET\n","    train = torch.load(train_folder)[:100]\n","    test = torch.load(test_folder)[:10]\n","    dataloader = torch.utils.data.DataLoader(train, batch_size=64,\n","                                                shuffle=True, num_workers=4)\n","    # DATASET for test\n","    # if you want to split train from test just move some files in another dir\n","    dataloader_test = torch.utils.data.DataLoader(test, batch_size=100,\n","                                                      shuffle=False, num_workers=1)\n","\n","    #margin and equilibirum\n","    margin = 0.35\n","    equilibrium = 0.68\n","    #mse_lambda = 1.0\n","    # OPTIM-LOSS\n","    # an optimizer for each of the sub-networks, so we can selectively backprop\n","    #optimizer_encoder = Adam(params=net.encoder.parameters(),lr = lr,betas=(0.9,0.999))\n","    optimizer_encoder = RMSprop(params=net.encoder.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n","    #lr_encoder = MultiStepLR(optimizer_encoder,milestones=[2],gamma=1)\n","    lr_encoder = ExponentialLR(optimizer_encoder, gamma=decay_lr)\n","    #optimizer_decoder = Adam(params=net.decoder.parameters(),lr = lr,betas=(0.9,0.999))\n","    optimizer_decoder = RMSprop(params=net.decoder.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n","    lr_decoder = ExponentialLR(optimizer_decoder, gamma=decay_lr)\n","    #lr_decoder = MultiStepLR(optimizer_decoder,milestones=[2],gamma=1)\n","    #optimizer_discriminator = Adam(params=net.discriminator.parameters(),lr = lr,betas=(0.9,0.999))\n","    optimizer_discriminator = RMSprop(params=net.discriminator.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n","    lr_discriminator = ExponentialLR(optimizer_discriminator, gamma=decay_lr)\n","    #lr_discriminator = MultiStepLR(optimizer_discriminator,milestones=[2],gamma=1)\n","\n","    batch_number = len(dataloader)\n","    step_index = 0\n","    widgets = [\n","\n","        'Batch: ', progressbar.Counter(),\n","        '/', progressbar.FormatCustomText('%(total)s', {\"total\": batch_number}),\n","        ' ', progressbar.Bar(marker=\"-\", left='[', right=']'),\n","        ' ', progressbar.ETA(),\n","        ' ',\n","        progressbar.DynamicMessage('loss_nle'),\n","        ' ',\n","        progressbar.DynamicMessage('loss_encoder'),\n","        ' ',\n","        progressbar.DynamicMessage('loss_decoder'),\n","        ' ',\n","        progressbar.DynamicMessage('loss_discriminator'),\n","        ' ',\n","        progressbar.DynamicMessage('loss_mse_layer'),\n","        ' ',\n","        progressbar.DynamicMessage('loss_kld'),\n","        ' ',\n","        progressbar.DynamicMessage(\"epoch\")\n","    ]\n","    # for each epoch\n","    if slurm:\n","        print(args)\n","    for i in range(n_epochs):\n","        print(i)\n","\n","        progress = progressbar.ProgressBar(min_value=0, max_value=batch_number, initial_value=0,\n","                                           widgets=widgets).start()\n","        # reset rolling average\n","        loss_nle_mean = RollingMeasure()\n","        loss_encoder_mean = RollingMeasure()\n","        loss_decoder_mean = RollingMeasure()\n","        loss_discriminator_mean = RollingMeasure()\n","        loss_reconstruction_layer_mean = RollingMeasure()\n","        loss_kld_mean = RollingMeasure()\n","        gan_gen_eq_mean = RollingMeasure()\n","        gan_dis_eq_mean = RollingMeasure()\n","        print(\"LR:{}\".format(lr_encoder.get_lr()))\n","\n","        # for each batch\n","        for j, data_batch in enumerate(dataloader):\n","            data_batch = data_batch / 255\n","            target_batch = data_batch\n","            \n","            # set to train mode\n","            net.train()\n","            # target and input are the same images\n","\n","            data_target = Variable(target_batch, requires_grad=False).float().cuda()\n","            data_in = Variable(data_batch, requires_grad=False).float().cuda()\n","\n","\n","            # get output\n","            out, out_labels, out_layer, mus, variances = net(data_in)\n","            # split so we can get the different parts\n","            out_layer_predicted = out_layer[:len(out_layer) // 2]\n","            out_layer_original = out_layer[len(out_layer) // 2:]\n","            # TODO set a batch_len variable to get a clean code here\n","            out_labels_original = out_labels[:len(out_labels) // 2]\n","            out_labels_sampled = out_labels[-len(out_labels) // 2:]\n","            # loss, nothing special here\n","            nle_value, kl_value, mse_value, bce_dis_original_value, bce_dis_sampled_value,\\\n","                bce_gen_original_value,bce_gen_sampled_value= VaeGan.loss(data_target, out, out_layer_original,\n","                                                                         out_layer_predicted, out_labels_original,\n","                                                                          out_labels_sampled, mus,\n","                                                                         variances)\n","            # THIS IS THE MOST IMPORTANT PART OF THE CODE\n","            loss_encoder = torch.sum(kl_value)+torch.sum(mse_value)\n","            loss_discriminator = torch.sum(bce_dis_original_value) + torch.sum(bce_dis_sampled_value)\n","            loss_decoder = torch.sum(lambda_mse * mse_value) - (1.0 - lambda_mse) * loss_discriminator\n","            # register mean values of the losses for logging\n","            loss_nle_mean(torch.mean(nle_value).data.cpu().numpy())\n","            loss_discriminator_mean((torch.mean(bce_dis_original_value) + torch.mean(bce_dis_sampled_value)).data.cpu().numpy())\n","            loss_decoder_mean((torch.mean(lambda_mse * mse_value) - (1 - lambda_mse) * (torch.mean(bce_dis_original_value) + torch.mean(bce_dis_sampled_value))).data.cpu().numpy())\n","\n","            loss_encoder_mean((torch.mean(kl_value) + torch.mean(mse_value)).data.cpu().numpy())\n","            loss_reconstruction_layer_mean(torch.mean(mse_value).data.cpu().numpy())\n","            loss_kld_mean(torch.mean(kl_value).data.cpu().numpy())\n","            # selectively disable the decoder of the discriminator if they are unbalanced\n","            train_dis = True\n","            train_dec = True\n","            if torch.mean(bce_dis_original_value).item() < equilibrium-margin or torch.mean(bce_dis_sampled_value).item() < equilibrium-margin:\n","                train_dis = False\n","            if torch.mean(bce_dis_original_value).item() > equilibrium+margin or torch.mean(bce_dis_sampled_value).item() > equilibrium+margin:\n","                train_dec = False\n","            if train_dec is False and train_dis is False:\n","                train_dis = True\n","                train_dec = True\n","\n","            #aggiungo log\n","            if train_dis:\n","                gan_dis_eq_mean(1.0)\n","            else:\n","                gan_dis_eq_mean(0.0)\n","\n","            if train_dec:\n","                gan_gen_eq_mean(1.0)\n","            else:\n","                gan_gen_eq_mean(0.0)\n","\n","            # BACKPROP\n","            # clean grads\n","            net.zero_grad()\n","            # encoder\n","            loss_encoder.backward(retain_graph=True)\n","            # someone likes to clamp the grad here\n","            #[p.grad.data.clamp_(-1,1) for p in net.encoder.parameters()]\n","            # update parameters\n","            optimizer_encoder.step()\n","            # clean others, so they are not afflicted by encoder loss\n","            net.zero_grad()\n","            #decoder\n","            if train_dec:\n","                loss_decoder.backward(retain_graph=True)\n","                #[p.grad.data.clamp_(-1,1) for p in net.decoder.parameters()]\n","                optimizer_decoder.step()\n","                #clean the discriminator\n","                net.discriminator.zero_grad()\n","            #discriminator\n","            if train_dis:\n","                loss_discriminator.backward()\n","                #[p.grad.data.clamp_(-1,1) for p in net.discriminator.parameters()]\n","                optimizer_discriminator.step()\n","\n","            # LOGGING\n","            # if  slurm:\n","            # progress.update(progress.value + 1, loss_nle=loss_nle_mean.measure,\n","            #                 loss_encoder=loss_encoder_mean.measure,\n","            #                 loss_decoder=loss_decoder_mean.measure,\n","            #                 loss_discriminator=loss_discriminator_mean.measure,\n","            #                 loss_mse_layer=loss_reconstruction_layer_mean.measure,\n","            #                 loss_kld=loss_kld_mean.measure,\n","            #                 epoch=i + 1)\n","\n","        # EPOCH END\n","        progress.update(progress.value + 1, loss_nle=loss_nle_mean.measure,\n","                        loss_encoder=loss_encoder_mean.measure,\n","                        loss_decoder=loss_decoder_mean.measure,\n","                        loss_discriminator=loss_discriminator_mean.measure,\n","                        loss_mse_layer=loss_reconstruction_layer_mean.measure,\n","                        loss_kld=loss_kld_mean.measure,\n","                        epoch=i + 1)\n","        lr_encoder.step()\n","        lr_decoder.step()\n","        lr_discriminator.step()\n","        margin *=decay_margin\n","        equilibrium *=decay_equilibrium\n","        #margin non puo essere piu alto di equilibrium\n","        if margin > equilibrium:\n","            equilibrium = margin\n","        lambda_mse *=decay_mse\n","        if lambda_mse > 1:\n","            lambda_mse=1\n","        progress.finish()\n","\n","        writer.add_scalar('loss_encoder', loss_encoder_mean.measure, step_index)\n","        writer.add_scalar('loss_decoder', loss_decoder_mean.measure, step_index)\n","        writer.add_scalar('loss_discriminator', loss_discriminator_mean.measure, step_index)\n","        writer.add_scalar('loss_reconstruction', loss_nle_mean.measure, step_index)\n","        writer.add_scalar('loss_kld',loss_kld_mean.measure,step_index)\n","        writer.add_scalar('gan_gen',gan_gen_eq_mean.measure,step_index)\n","        writer.add_scalar('gan_dis',gan_dis_eq_mean.measure,step_index)\n","\n","    for j, data_batch in enumerate(dataloader_test):\n","        data_batch = data_batch / 255\n","        target_batch = data_batch\n","        net.eval()\n","\n","        data_in = Variable(data_batch, requires_grad=False).float().cuda()\n","        data_target = Variable(target_batch, requires_grad=False).float().cuda()\n","        out = net(data_in)\n","        out = out.data.cpu()\n","        out = (out + 1) / 2\n","        out = make_grid(out, nrow=8)\n","        save_image(out.data.cpu(), '{}/neurosmash/images_gan/sample_image_epoch_{}_rec.png'.format(base_url, j))\n","\n","        img = mpimg.imread('{}/neurosmash/images_gan/sample_image_epoch_{}_rec.png'.format(base_url, j))\n","        imgplot = plt.imshow(img)\n","        plt.show()\n","        # out = net(None, 100)\n","        # out = out.data.cpu()\n","        # out = (out + 1) / 2\n","        # out = make_grid(out, nrow=8)\n","        # writer.add_image(\"generated\", out, step_index)\n","\n","        out = data_target.data.cpu()\n","        out = (out + 1) / 2\n","        out = make_grid(out, nrow=8)\n","        save_image(out.data.cpu(), '{}/neurosmash/images_gan/sample_image_epoch_{}_orig.png'.format(base_url, j))\n","        break\n","\n","        step_index += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M0qklfnJ2uf6","colab_type":"text"},"source":["## PCA"]},{"cell_type":"code","metadata":{"id":"sKBeza_w2v8D","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","from sklearn.decomposition import PCA\n","import numpy as np \n","import torch \n","import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","import pickle\n","import time\n","\n","X = np.transpose(torch.load('{}/data/training_data_0.pt'.format(base_url)).data.numpy(), (0, 2, 3, 1))\n","X = X[:,:,:, 0]\n","X = X.reshape(X.shape[0], -1)\n","# test = torch.load('{}/data/training_data_1.pt'.format(base_url)).data.numpy()\n","start = time.time()\n","pca = PCA(4, svd_solver='randomized').fit(X)\n","print(time.time() - start)\n","filename = '{}/PCA_model.pkl'.format(base_url)\n","pickle.dump(pca, open(filename, 'wb'))\n","\n","# X_proj = pca.transform(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jO3XymZ-7CG","colab_type":"code","colab":{}},"source":["np.sum(pca.explained_variance_ratio_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxUn6LieEF9t","colab_type":"code","colab":{}},"source":["X_proj.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5uq34kEBBU4","colab_type":"code","colab":{}},"source":["X_proj = pca.transform(X)\n","temp = pca.inverse_transform(X_proj)\n","temp = np.reshape(temp, (-1, 64, 64))\n","temp.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pN3Isfw-rrf","colab_type":"code","colab":{}},"source":["fig = plt.figure(figsize=(8,8)) \n","fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n","# plot the faces, each image is 64 by 64 pixels \n","for i in range(10): \n","    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n","    ax.imshow(temp[i], cmap=plt.cm.bone, interpolation='nearest')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GweT7uek4Pa7","colab_type":"code","colab":{}},"source":["fig = plt.figure(figsize=(8,8)) \n","fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n","for i in range(10):\n","    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[]) \n","    ax.imshow(X[i].reshape(64, 64), cmap=plt.cm.bone, interpolation='nearest')"],"execution_count":0,"outputs":[]}]}